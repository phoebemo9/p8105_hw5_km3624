---
title: "Homework 3"
author: "Phoebe Mo"
date: "2020-10-08"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 0.6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

### Problem 1
```{r}
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns. Observations are the level of items in orders by user. There are
user/order variables -- user ID, order ID, order date and order hour. There
are also item variable -- name, aisle, department, and some numeric codes.

How many aisles, and which are most items from?
```{r}
instacart %>%
  count(aisle) %>%
  arrange(desc(n))
```

Let's make a plot
```{r}
instacart %>%
  count(aisle) %>%
  filter(n > 10000) %>%
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>%
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

Let's make a table
```{r}
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>%
  count(product_name) %>%
  mutate(rank = min_rank(desc(n))) %>%
  filter(rank < 4) %>%
  arrange(aisle, rank) %>%
  knitr::kable()
```

Apple vs ice cream..

```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```

### Problem 2

Load, tidy, and otherwise wrangle the data

```{r}
accel_df =
  read.csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>%
  pivot_longer(
    activity_1:activity_1440,
    names_prefix = "activity_",
    names_to = "min_of_day",
    values_to = "activity_count"
  ) %>%
  mutate(
    dow = ifelse(day == "Saturday", "weekend",
          ifelse(day == "Sunday", "weekend", "weekday")),
    min_of_day = as.numeric(min_of_day)
  )
```
This resulting dataset has totally `r nrow(accel_df)` observations, each is about the
activity count in each minute of a day in a specific week. There are variables: week,
day_id, day, dow(day of week), minute of the day, and the activity counts. There are totally five weeks and 35 days in this dataset, and the mean activity count in these 35 days is `r mean(pull(accel_df, activity_count))`

Now create a table showing total activity over the day

```{r}
accel_df %>%
  group_by(week, day) %>%
  summarize(
    activity_daily = sum(activity_count)
  ) %>%
  pivot_wider(
    names_from = "day",
    values_from = "activity_daily"
  ) %>%
  select("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday") %>%
  knitr::kable()
```

Observing the table, it is significant that in week 1, there are less activity counts, especially in weekdays and Saturday. Also, the activity counts during Tuesday and Wednesday are relatively stable. The activity counts on Sunday is mostly decreasing during these 5 weeks.

Now, make a plot showing 24-hour activity time courses for each day

```{r}
accel_df %>%
  group_by(day) %>%
  ggplot(aes(x = min_of_day, y = activity_count, color = day)) +
  geom_line(alpha = 0.5) +
  theme(legend.position = "bottom", plot.title = element_text(size = 15, hjust = 0.5)) +
  labs(
    title = "Activity during a day",
    x = "minute (min)",
    y = "activity counts"
  )
```

Observing the plot, we can find that during a day, the activity count starts to increase from approximately 270 min of the beginning of every day. On Friday, the patient has significant higher activity at around 8pm and on Sunday, there is much more activity at around noon. For other days, their activity count is relatively stable during a day after the early morning.

### Problem 3

```{r}
library(p8105.datasets)
data("ny_noaa")
```

This ny_noaa dataset contains `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. Observations are the weather situation of a specific station in each day from the year 1981 to the year 2010. There are weather related variables -- precipitation, snowfall, snow depth, tmax and tmin. There are also item variables -- station ID, date of observation.

In this dataset, there are many missing values within. For example, there are `r sum(is.na(ny_noaa$tmax))` missing values in tmax, which is near to 44% of the observations. Since there are a large number of missing values in some variables, they may cause a reduce in statistical power and may cause some bias when we are analyzing.

```{r}
ny_noaa_df =
  ny_noaa %>%
  janitor::clean_names() %>%
  separate(
    date,
    into = c("year", "month", "day"),
    sep = "-",
  ) %>%
  mutate(
    tmax = as.numeric(tmax),
    tmin = as.numeric(tmin),
  ) %>%
  mutate(
    tmax = tmax / 10,
    tmin = tmin / 10
  )

ny_noaa %>%
  count(snow) %>%
  arrange(desc(n))
```

For snowfall, the most common observed value is 0, where there are 2008508 records showing 0. Then, there are 381221 missing values. The most common value is 0 is because during a year, NY will have snow in winter, and there is no snow in other three seasons.

Now, make a two-panel plot showing the average max temperature in January and July in each station across years.

```{r}
plot_temp =
  ny_noaa_df %>%
  filter(month == c("01", "07")) %>%
  filter(!is.na(tmax)) %>%
  mutate(
    year = as.numeric(year)
  ) %>%
  group_by(year, month, id) %>%
  summarize(mean_max = mean(tmax, na.rm = T)) %>%
  ggplot(aes(x = year, y = mean_max, color = id, group = id)) +
  geom_point(alpha = 0.5) +
  geom_path(alpha = 0.5) +
  facet_grid(. ~ month) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), legend.position = "none", plot.title = element_text(size = 15, hjust = 0.5)) +
  scale_x_continuous(breaks = seq(from = 1980, to = 2010, by = 5)) +
  labs(
    title = "Average max temperature in January",
    x = "Year",
    y = "Average Max Temperature"
  )

plot_temp
```

Observing the plot, the average max temperature is much greater than that of January. We can also know that in January, current year's average of max is increasing. That is, the average max temperature is increasing in the winter case. The structure is like a wave-shape, which means the average max temperature fluctuates in these 30 years. There is an outlier in January of 1982, in which the average max temperture is lower than -12.5.

For July, the average of max temperature is also increasing, and this is apparent in 2010. We can also observe that this plot has a relatively stable wave-like structure than the previous one. In approximately 4-year interval, the average max temperature in July will decrease relatively sharp in a time period, and there is an outlier in July of 1988, which is lower than 15.

Now make a two-panel plot showing tmax vs tmin for the full dataset
```{r fig.asp = 1.2}
temp_compare_plot =
  ny_noaa_df %>%
  ggplot(aes(x = tmax, y = tmin)) +
  geom_hex(na.rm = T) +
  theme(legend.text = element_text(angle = 90, vjust = 0.5, hjust = 0.5),      plot.title = element_text(size = 15, hjust = 0.5)) +
  labs(title = "tmax vs tmin")

snowfall_plot =
  ny_noaa_df %>%
  filter(!is.na(snow), snow > 0, snow < 100) %>%
  ggplot(aes(x = year, y = snow)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1), plot.title = element_text(size = 15, hjust = 0.5)) +
  labs(title = "snowfall in 1981-2010", x = "Year", y = "snowfall")

temp_compare_plot / snowfall_plot
```

In the plot comparing tmax and tmin, most data is gathering around hexs where the tmax is around 10-20 and tmin is around 15. In the annual snowfall plot,
we notice that the median and interquatile range is approximately same for each year except for 1998, 2004, 2006, 2007, and 2010. In the plot, there are also some outliers in year 1998, 2006, and 2010. It is interesting to notice that after 2004, the snowfall level fluctuated more between years and began to have lower minimum snowfall compared to years before 2004.
